enable_ansible_debug: true # set value to true for verbose output from ansible

# format: "http://<jumphost_ip>:40001"
nsx_image_webserver: "http://10.173.61.30:40001"
#ova_file_name: "nsx-unified-appliance-2.2.0.0.0.8680778.ova" #Uncomment this if downloaded file manually and placed under /home/concourse
#ovftool_file_name: "VMware-ovftool-4.2.0-5965791-lin.x86_64.bundle"   #Uncomment this if downloaded file manually and placed under /home/concourse

# vCenter to deploy the NSX manager
vcenter_ip: 10.173.61.22
vcenter_username: administrator@vsphere.local
vcenter_password: "VMware1!"
vcenter_datacenter: Datacenter
vcenter_cluster: PKS-MGMT      #management cluster
vcenter_datastore: ISCSI-DATASTORE

# NSX manager general network settings
mgmt_portgroup: 'MGMT-PG'
dns_server: 10.173.61.33
dns_domain: mylab.local
ntp_servers: 10.173.61.33
default_gateway: 10.173.61.253
netmask: 255.255.255.0

nsx_manager_ip: 10.173.61.23
nsx_manager_username: admin
nsx_manager_password: "VMware1!"
nsx_manager_assigned_hostname: "manager" # this hostname+dns_domain will be FQDN
nsx_manager_root_pwd: "VMware1!"    # Min 8 chars, upper, lower, number, special digit
nsx_manager_deployment_size: small   # Recommended for real barebones demo, smallest setup
nsx_manager_ssh_enabled: true
resource_reservation_off: true

# Compute manager credentials should be the same as above vCenter's if
# controllers and edges are to be on the same vCenter
compute_manager_username: "administrator@vsphere.local"
compute_manager_password: "VMware1!"
# compute manager for the compute cluster (2nd vCenter)
compute_manager_2_vcenter_ip: "null"
compute_manager_2_username: "null"
compute_manager_2_password: "null"

edge_uplink_profile_vlan: 0 # For outbound uplink connection used by Edge, usually keep as 0
esxi_uplink_profile_vlan: 1547 # For internal overlay connection used by Esxi hosts, usually trasnport VLAN ID

# Virtual Tunnel Endpoint network ip pool
vtep_ip_pool_cidr: 192.168.213.0/24
vtep_ip_pool_gateway: 192.168.213.1
vtep_ip_pool_start: 192.168.213.10
vtep_ip_pool_end: 192.168.213.200

# Tier 0 router
tier0_router_name: DefaultT0Router
tier0_uplink_port_ip: 10.173.61.35    # putting dummy IP address here from the different subnet as the snat as it is failing NAT rules creation
tier0_uplink_port_subnet: 24
tier0_uplink_next_hop_ip: 10.173.61.33
tier0_uplink_port_ip_2:
tier0_ha_vip:

## Controllers
controller_ips: 10.173.61.24 #comma separated based on number of required controllers
controller_default_gateway: 10.173.61.253
controller_ip_prefix_length: 24
controller_hostname_prefix: controller # Generated hostname: controller-1.corp.local.io
controller_cli_password: "VMware1!" # Min 8 chars, upper, lower, num, special char
controller_root_password: "VMware1!"
controller_deployment_size: "SMALL"
vc_datacenter_for_controller: Datacenter
vc_cluster_for_controller: PKS-MGMT
vc_datastore_for_controller: ISCSI-DATASTORE
vc_management_network_for_controller: "MGMT-PG"
controller_shared_secret: "VMware1!"

## Edge nodes
edge_ips: 10.173.61.25    #comma separated based in number of required edges
edge_default_gateway: 10.173.61.253
edge_ip_prefix_length: 24
edge_hostname_prefix: edge
edge_transport_node_prefix: edge-transport-node
edge_cli_password: "VMware1!"
edge_root_password: "VMware1!"
edge_deployment_size: "large" #Large recommended for PKS deployments
vc_datacenter_for_edge: Datacenter
vc_cluster_for_edge: PKS-MGMT
vc_datastore_for_edge: ISCSI-DATASTORE
vc_uplink_network_for_edge: "MGMT-PG"
vc_overlay_network_for_edge: "EDGE-Overlay"
vc_management_network_for_edge: "MGMT-PG"

## ESX hosts
#Intsll vSphere clusters automatically
clusters_to_install_nsx: PKS-COMP    #Comma seprated
per_cluster_vlans: 1547  #Comma seprated, order of VLANs applied same as order of clusters

esx_ips: "" # additional esx hosts, if any, to be individually installed
esx_os_version: "6.5.0"
esx_root_password: "ca$hc0w"
esx_hostname_prefix: "esx-host"

esx_available_vmnic: vmnic1 # comma separated physical NICs, applies to both cluster installation or ESXi installation

nsx_t_t1router_logical_switches_spec: |
  t1_routers:
  # Add additional T1 Routers or collapse switches into same T1 Router as needed
  - name: T1-PKS-Infra
    switches:
    - name: PKS-Infra
      logical_switch_gw: 10.173.61.253 # Last octet should be 1 rather than 0
      subnet_mask: 24

  t1_routers:
  # T1 for PKS-Services - place holder (not used but PKS pipeline will fail if it is not defined)
  - name: T1-PKS-Services
    switches:
    - name: PKS-Services
      logical_switch_gw: 172.15.0.1 # Last octet should be 1 rather than 0
      subnet_mask: 24

nsx_t_container_ip_block_spec: |
  container_ip_blocks:
  - name: PKS-node-ip-block
    cidr: 10.1.0.0/16

  - name: PKS-pod-ip-block
    cidr: 10.2.0.0/16


nsx_t_external_ip_pool_spec: |
  external_ip_pools:

  - name: snat-vip-pool-for-pks
    cidr: 23.23.23.0/24
    start: 23.23.23.10 # Should not include gateway
    end: 23.23.23.200  # Should not include gateway


nsx_t_nat_rules_spec: |
  nat_rules:

nsx_t_csr_request_spec: |
  csr_request:
    #common_name not required - would use nsx_t_manager_host_name
    org_name: Company            # EDIT
    org_unit: net-integ          # EDIT
    country: US                  # EDIT
    state: CA                    # EDIT
    city: SF                     # EDIT
    key_size: 2048               # Valid values: 2048 or 3072
    algorithm: RSA               # Valid values: RSA or DSA


nsx_t_lbr_spec: |
  loadbalancers:

nsx_t_ha_switching_profile_spec: |
  ha_switching_profiles:

